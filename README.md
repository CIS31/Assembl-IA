# Assembl-IA

PrÃ©sentation du projet
  
PrÃ©sentation de la pipeline Azure  

![Demo](./assets/Fil%20rouge%20v2.png)

## Analyse textuelle

## Analyse audio

#### ğŸš€ Introduction
Ce projet met en Å“uvre un pipeline complet d'analyse audio, axÃ© sur l'extraction et l'Ã©tude des caractÃ©ristiques prosodiques du langage parlÃ©, ainsi que sur la diarisation des locuteurs. L'objectif est de fournir des informations dÃ©taillÃ©es sur "qui a parlÃ© quand" et "comment" (en termes de hauteur et d'intensitÃ© de la voix), facilitant ainsi une comprÃ©hension approfondie des interactions vocales.

#### ğŸ¯ Objectifs de l'Analyse
Notre analyse audio vise Ã  atteindre les objectifs suivants :

* **Compter le nombre de locuteurs** distincts prÃ©sents dans un enregistrement audio.
* RÃ©aliser la **Diarisation des Locuteurs** : identifier prÃ©cisÃ©ment les segments oÃ¹ chaque locuteur a pris la parole (dÃ©terminer "qui a parlÃ© quand").
* Extraire et analyser l'**IntensitÃ©** (volume sonore) de la voix pour chaque locuteur.
* Extraire et analyser le **Pitch (F0)** (frÃ©quence fondamentale / hauteur de la voix) pour chaque locuteur.
* GÃ©nÃ©rer une **Timeline de Prise de Parole** : visualiser graphiquement les pÃ©riodes d'activitÃ© vocale de chaque participant.

#### âš™ï¸ MÃ©thodologie

Le pipeline d'analyse est structurÃ© en plusieurs Ã©tapes clÃ©s :

1.  **Chargement et PrÃ©paration des DonnÃ©es :**
    * **Audio :** L'enregistrement audio est chargÃ© Ã  l'aide de la bibliothÃ¨que `librosa`, garantissant une frÃ©quence d'Ã©chantillonnage et un format mono cohÃ©rents pour l'analyse.
    * **Diarisation (via XML) :** Les segments de prise de parole des locuteurs sont importÃ©s depuis un fichier XML (avec une structure prÃ©dÃ©finie ` <segment start="..." end="..." speaker="..."/>`). Ces donnÃ©es sont ensuite traitÃ©es pour Ãªtre utilisÃ©es dans l'analyse.

2.  **Conversion au Format RTTM :**
    * Les informations de diarisation extraites du fichier XML peuvent Ãªtre converties et exportÃ©es au format standard **RTTM** (Rich Transcription Time Marked). Ce format est largement utilisÃ© dans le domaine du traitement de la parole pour reprÃ©senter les segments de locuteurs, facilitant l'interopÃ©rabilitÃ© et l'Ã©valuation.

3.  **Extraction des CaractÃ©ristiques Prosodiques :**
    * Pour chaque segment de parole identifiÃ© par la diarisation, les caractÃ©ristiques prosodiques suivantes sont extraites :
        * **Pitch (F0) :** La frÃ©quence fondamentale de la voix est calculÃ©e en utilisant `librosa.core.piptrack`. Des filtres sont appliquÃ©s pour assurer la fiabilitÃ© des mesures de pitch (en excluant les valeurs avec une faible magnitude).
        * **IntensitÃ© :** L'Ã©nergie RMS (Root Mean Square) est calculÃ©e via `librosa.feature.rms` et convertie en dÃ©cibels (dB), fournissant une mesure du volume sonore.
    * Ces caractÃ©ristiques sont extraites sous forme de **contours temporels** (l'Ã©volution des valeurs au fil du temps) et de donnÃ©es brutes pour des analyses statistiques.

4.  **Analyse et AgrÃ©gation des DonnÃ©es :**
    * Les donnÃ©es de pitch et d'intensitÃ© sont regroupÃ©es par locuteur.
    * Des statistiques descriptives clÃ©s (moyenne, mÃ©diane, Ã©cart-type) pour le pitch et l'intensitÃ© sont calculÃ©es pour chaque locuteur.
    * La durÃ©e totale de parole de chaque locuteur est comptabilisÃ©e pour Ã©valuer leur contribution.

#### ğŸ“Š Sorties et Visualisation

Le projet gÃ©nÃ¨re plusieurs types de sorties, principalement sous forme de fichiers CSV pour faciliter l'intÃ©gration avec des outils de visualisation ou d'analyse externe :

* **Fichier RTTM** (`.rttm`) : Diarisation des locuteurs au format standard.
* **Timeline des Locuteurs** (`timeline_data.csv`) : Un fichier CSV dÃ©taillant chaque segment de parole avec `speaker_label`, `start`, `duration` et `end`. Pour faciliter l'affichage graphique, une sÃ©lection des locuteurs les plus actifs (par exemple, le top 5) est gÃ©nÃ©ralement privilÃ©giÃ©e pour la visualisation directe, mais toutes les donnÃ©es sont disponibles dans ce fichier.
* **Distributions Prosodiques** (`prosody_stats.csv`) : Un fichier CSV rÃ©capitulant les statistiques clÃ©s (moyenne, mÃ©diane, Ã©cart-type) du pitch et de l'intensitÃ© pour chaque locuteur.
* **Contours de Pitch** (`pitch_contours.csv`) : Un fichier CSV contenant les donnÃ©es de sÃ©ries temporelles pour le contour de pitch (`speaker_label`, `time_s`, `pitch_hz`).
* **Contours d'IntensitÃ©** (`intensity_contours.csv`) : Un fichier CSV contenant les donnÃ©es de sÃ©ries temporelles pour le contour d'intensitÃ© (`speaker_label`, `time_s`, `intensity_db`).

Des fonctions de traÃ§age sont Ã©galement incluses pour gÃ©nÃ©rer des reprÃ©sentations visuelles (diagrammes de timeline, histogrammes de distribution, tracÃ©s de contours) directement si nÃ©cessaire.

#### ğŸ› ï¸ Utilisation (Conceptuel)

Pour utiliser ce pipeline, vous devrez gÃ©nÃ©ralement :

1.  Avoir un fichier audio (ex: `.wav`).
2.  Disposer d'un fichier XML de retranscription associÃ©.
3.  ExÃ©cuter la classe d'analyse fournie en lui passant ces fichiers en entrÃ©e.
4.  Les fichiers de sortie CSV seront gÃ©nÃ©rÃ©s dans le rÃ©pertoire spÃ©cifiÃ© (par dÃ©faut `./output/`).

#### ğŸ“Š Analyse Interactive et Visualisations

![prosodic_contour](./audio/output/prosodic_contours_speaker_x.png)

Pour une exploration interactive des donnÃ©es et la visualisation des diffÃ©rentes sorties graphiques (timeline, contours de pitch et d'intensitÃ©, distributions), un **Notebook Jupyter** est fourni :

* **`audio_analysis.ipynb`** : Ce notebook contient le code pas Ã  pas qui gÃ©nÃ¨re les donnÃ©es exportÃ©es et illustre comment crÃ©er les diffÃ©rents graphiques mentionnÃ©s (timeline, contours, distributions de pitch et d'intensitÃ©) Ã  partir de ces donnÃ©es. Il sert de guide pratique pour comprendre le fonctionnement de l'analyse et interprÃ©ter les rÃ©sultats visuellement.


![distribution_intensitÃ©](./audio/output/distribution_intensitÃ©.png)


#### PrÃ©sentation

Cette brique du projet vise Ã  **dÃ©tecter les Ã©motions dans des textes en franÃ§ais** (transcriptions audio, commentaires, scripts, etc.).  
Afin dâ€™aligner la sortie textuelle sur lâ€™axe visuel, nous couvrons **7 Ã©motions** : Tristesse (`sad`), Peur (`fear`), ColÃ¨re (`anger`), Neutre (`neutral`), Surprise (`surprise`), Joie (`joy`) et DÃ©goÃ»t (`disgusted`).

#### FonctionnalitÃ©s

- âœ… PrÃ©-traitement complet du texte (nettoyage, normalisation, tokenisation)
- âœ… Classification des Ã©motions sur 7 classes
- âœ… Export CSV contenant : timestamp, texte original, Ã©motion prÃ©dite, score de confiance
- âœ… IntÃ©gration directe dans la pipeline Azure (Databricks + Blob Storage + Postgres)

#### Pipeline Azure

1. Lecture des fichiers texte ou transcriptions stockÃ©s dans le **Blob Storage**  
2. **Databricks** appelle le notebook de prÃ©diction NLP  
3. Les prÃ©dictions sont :
   - stockÃ©es au format CSV dans le Blob Storage (dossier *output*)  
   - insÃ©rÃ©es dans **Postgres** pour exploitation BI  
4. Les mÃ©triques dâ€™exÃ©cution (latence, nombre de tokens) sont remontÃ©es Ã  **Azure Application Insights**

#### ModÃ¨le utilisÃ©

| Nom | Base | Type | Lien |
|-----|------|------|------|
| `assembl-ia/french_emotion_camembert-7cls` | CamemBERT-base | Fine-tune (7 Ã©motions) | ğŸ”— [Hugging Face (original)](https://huggingface.co/astrosbd/french_emotion_camembert) |

- **Fine-tuning** rÃ©alisÃ© sur un jeu de donnÃ©es Ã©quilibrÃ© de **5 033 phrases** (719 exemples/Ã©motion).  
- **Epochs** : 5 â€ƒâ€ƒâ€¢â€ƒâ€ƒ**Batch size** : 16 â€ƒâ€ƒâ€¢â€ƒâ€ƒ**LR** : 2e-5

#### DonnÃ©es dâ€™entraÃ®nement

| Source | Langue | Taille | ParticularitÃ© |
|--------|--------|--------|---------------|
| **EMODIFT** | FR | 3 194 | AnnotÃ© manuellement |
| **TPM-28 / emotion-FR** | FR | 1 839 | Contient la classe *DÃ©goÃ»t* |

Les deux jeux ont Ã©tÃ© fusionnÃ©s puis rÃ©-Ã©quilibrÃ©s pour obtenir exactement **719 exemples / classe**.

#### Ã‰valuation du modÃ¨le

| Emotion   | PrÃ©cision |
|-----------|-----------|
| Angry     | 0.87      |
| Disgust   | 0.83      |
| Fear      | 0.91      |
| Happy     | 0.93      |
| Neutral   | 0.86      |
| Sad       | 0.94      |
| Surprise  | 0.99      |

**PrÃ©cision globale** : **0.90**

![Confusion Matrix](./assets/confusion_matrix_text_model.png)

> Le score de 90 % sur le set de test confirme lâ€™intÃ©rÃªt du fine-tuning pour ajouter la classe *DÃ©goÃ»t* (vs 82 % avant adaptation).


## Analyse audio

## Analyse vidÃ©o

#### PrÃ©sentation

Il s'agit d'un gif, la vidÃ©o au format .mp4 est disponible dans le dossier output

![Demo](./assets/video_vitrine.gif)


##  FonctionnalitÃ©s

- âœ… Lecture vidÃ©o frame par frame
- âœ… DÃ©tection des visages
- âœ… Si visage assez grand â†’ DÃ©tection des Ã©motions (les 2 classes majoritaires)
- âœ… Annotation des rÃ©sultats sur la vidÃ©o en output
- âœ… CrÃ©ation d'un timeline (fichier CSV)

## Pipeline Azure

- âœ… Lecture des variables d'environnement contenues dans les paramÃ¨tres du job databricks
- âœ… RecupÃ©ration de la derniÃ¨re vidÃ©o prÃ©sente sur le blob storage
- âœ… Traitement
- âœ… Enregistrement de la vidÃ©o annotÃ©e et de la timeline dans le blob storage
- âœ… Enregistrement de la timeline dans postgres

##  ModÃ¨les utilisÃ©s

- YOLO v8 : 
ğŸ”— https://yolov8.com/

- facial_emotions_image_detection : 
ğŸ”— https://huggingface.co/dima806/facial_emotions_image_detection

##  Evaluation des modÃ¨les 

- utilisation du dataset de test suivant :
ğŸ”— https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer

- resultats : 
![Demo](./assets/testdumodelvideo.png)
  
| Emotion   | PrÃ©cision |
|-----------|-----------|
| Angry     | 0.772     |
| Disgust   | 1.000     |
| Fear      | 0.838     |
| Happy     | 0.822     |
| Neutral   | 0.740     |
| Sad       | 0.943     |
| Surprise  | 0.928     |

PrÃ©cision globale : **0.847**

